{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577c8c7b",
   "metadata": {},
   "source": [
    "# Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a70eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7338c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the duration and sample rate for recording:\n",
    "duration = 5  # Set the duration of the recording (in seconds)\n",
    "sample_rate = 22050  # Set the sample rate (commonly used for audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45b41f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Recording started. Speak into the microphone...\")\n",
    "recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7617883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved to recorded_audio.wav.\n"
     ]
    }
   ],
   "source": [
    "#Save the recorded audio to a file:\n",
    "audio_file = \"recorded_audio.wav\"  # Choose a filename for the recorded audio\n",
    "sf.write(audio_file, recording, sample_rate)\n",
    "print(f\"Audio saved to {audio_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af0f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract audio features for classification:\n",
    "# Load the audio file\n",
    "audio, sr = librosa.load(audio_file, sr=sample_rate)\n",
    "\n",
    "# Extract audio features\n",
    "features = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "features = features.reshape(1, -1)  # Reshape features to match the expected input shape of the MLP classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6eb60b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the saved model from a file\n",
    "loaded_model = joblib.load('model_file.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1aaac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: ['calm']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'features' is your extracted features with shape (n_samples, 128)\n",
    "expected_shape = 180  # Expected shape of the MLPClassifier input\n",
    "\n",
    "if features.shape[1] < expected_shape:\n",
    "    padding_width = expected_shape - features.shape[1]\n",
    "    padded_features = np.pad(features, ((0, 0), (0, padding_width)), mode='constant')\n",
    "elif features.shape[1] > expected_shape:\n",
    "    padded_features = features[:, :expected_shape]  # Truncate to expected shape\n",
    "else:\n",
    "    padded_features = features  # No padding or truncation needed\n",
    "\n",
    "# Reshape features to match the expected input shape of the MLP classifier\n",
    "reshaped_features = padded_features.reshape(padded_features.shape[0], -1)\n",
    "\n",
    "# Use the loaded MLP classifier to predict the emotion\n",
    "emotion = loaded_model.predict(reshaped_features)\n",
    "print(\"Predicted emotion:\", emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a7f09cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: ['happy']\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with sf.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        return result\n",
    "\n",
    "# Specify the path to the recorded audio file\n",
    "file_path = \"recorded_audio.wav\"\n",
    "\n",
    "# Extract features from the recorded audio\n",
    "features = extract_feature(file_path, mfcc=True, chroma=True, mel=True)\n",
    "\n",
    "# Pad or truncate the features to match the expected shape\n",
    "expected_shape = 180  # Expected shape of the MLPClassifier input\n",
    "padded_features = np.pad(features, (0, max(0, expected_shape - len(features))), mode='constant')\n",
    "\n",
    "# Reshape the features to match the expected input shape of the MLP classifier\n",
    "reshaped_features = padded_features.reshape(1, -1)\n",
    "\n",
    "# Use the loaded MLP classifier to predict the emotion\n",
    "emotion = loaded_model.predict(reshaped_features)\n",
    "print(\"Predicted emotion:\", emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3edb338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n",
      "Predicted emotion: ['fearful']\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with sf.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        return result\n",
    "\n",
    "# Specify the path to save the recorded audio file\n",
    "file_path = \"recorded_audio.wav\"\n",
    "\n",
    "# Specify the duration and sample rate of the recording\n",
    "duration = 5  # Duration of the recording in seconds\n",
    "sample_rate = 22050  # Sample rate of the recording\n",
    "\n",
    "# Start recording\n",
    "print(\"Recording started. Speak into the microphone...\")\n",
    "recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording completed.\")\n",
    "\n",
    "# Save the recorded audio\n",
    "sf.write(file_path, recording, sample_rate)\n",
    "\n",
    "# Extract features from the recorded audio\n",
    "features = extract_feature(file_path, mfcc=True, chroma=True, mel=True)\n",
    "\"\"\n",
    "# Pad or truncate the features to match the expected shape\n",
    "expected_shape = 180  # Expected shape of the MLPClassifier input\n",
    "padded_features = np.pad(features, (0, max(0, expected_shape - len(features))), mode='constant')\n",
    "\n",
    "# Reshape the features to match the expected input shape of the MLP classifier\n",
    "reshaped_features = padded_features.reshape(1, -1)\n",
    "\n",
    "# Use the loaded MLP classifier to predict the emotion\n",
    "emotion = loaded_model.predict(reshaped_features)\n",
    "print(\"Predicted emotion:\", emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
